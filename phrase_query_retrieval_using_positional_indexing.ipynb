{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "# from nltk.corpus import stopwords -- we are not removing stop words because they also play role for postional indexing\n",
    "# stop_words = set(stopwords.words('english')) \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import pickle\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For loading and storing Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_data(filename,file):\n",
    "    filename = os.path.join('./pickle',filename)\n",
    "    out = open(filename, 'wb')\n",
    "    pickle.dump(file, out)\n",
    "    out.close()\n",
    "    \n",
    "def load_pickle_data(file):\n",
    "    file = os.path.join('./pickle',file)\n",
    "    out = open(file, 'rb')\n",
    "    index_dict1 = pickle.load(out)\n",
    "    out.close()\n",
    "    return index_dict1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. During Pre Processing :\n",
    "    + We remove meta data\n",
    "    + Removing un necessary symbols in the words\n",
    "    + Removing all the tokens except alpha numeric tokens\n",
    "    + Removing all tokens that are less than length 2\n",
    "    + Lemmatization of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_metadata(lines):\n",
    "    for i in range(len(lines)):\n",
    "        if lines[i] == '\\n':\n",
    "            start = i + 1\n",
    "            break\n",
    "    return lines[start:]\n",
    "\n",
    "def process_words(words):\n",
    "    mod_words = []\n",
    "    symbols = [ \"'\",'/','.','-','!','@','#','$','^','&','*','(',')','+']\n",
    "    #words = [word.replace(sym,'') for word in words for sym in symbols if sym in word]\n",
    "    removed_symbols = []\n",
    "    for word in words :\n",
    "        for sym in symbols:\n",
    "            if sym in word:\n",
    "                word = word.replace(sym,'')\n",
    "        removed_symbols.append(word)\n",
    "        \n",
    "    words = removed_symbols\n",
    "    del removed_symbols\n",
    "        \n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word.isalnum():\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            #if word not in stop_words and len(word) > 2:\n",
    "            if len(word) >= 2:\n",
    "                mod_words.append(word)\n",
    "    return mod_words\n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "def process_text(lines, positional_dict,doc_ID):\n",
    "    ''' This method is handle all the text in the file.\n",
    "    \n",
    "        This will remove meta data, pre process the file and construct the positional dictionary.\n",
    "        \n",
    "        positional dictionary format :\n",
    "            pos_dict = {\n",
    "                        token : [doc_freq,{\n",
    "                            file_id:[pos1, pos2,.....]\n",
    "                            }]\n",
    "                        }\n",
    "    '''\n",
    "    lines = remove_metadata(lines)\n",
    "    seperator = ' '\n",
    "    file = seperator.join(lines)\n",
    "    words = word_tokenize(file)\n",
    "    words = process_words(words)\n",
    "    for pos,word in enumerate(words):\n",
    "        if word in positional_dict.keys():\n",
    "            positional_dict[word][0] += 1 # doc frequency of word \n",
    "            if doc_ID in positional_dict[word][1].keys():\n",
    "                positional_dict[word][1][doc_ID].append(pos)\n",
    "            else:\n",
    "                positional_dict[word][1][doc_ID] = [pos] # adding the new file id and its position\n",
    "            \n",
    "        else :\n",
    "            dummy_dict = {}\n",
    "            dummy_dict[doc_ID] = [pos]\n",
    "            positional_dict[word] = [1, dummy_dict] # freq is 1 and its corresponding dictionary\n",
    "            \n",
    "    return positional_dict\n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files in : ./data/rec.motorcycles\n",
      "Processing of 100 files in ./data/rec.motorcycles is completed \n",
      "Processing of 200 files in ./data/rec.motorcycles is completed \n",
      "Processing of 300 files in ./data/rec.motorcycles is completed \n",
      "Processing of 400 files in ./data/rec.motorcycles is completed \n",
      "Processing of 500 files in ./data/rec.motorcycles is completed \n",
      "Processing of 600 files in ./data/rec.motorcycles is completed \n",
      "Processing of 700 files in ./data/rec.motorcycles is completed \n",
      "Processing of 800 files in ./data/rec.motorcycles is completed \n",
      "Processing of 900 files in ./data/rec.motorcycles is completed \n",
      "Processing of 1000 files in ./data/rec.motorcycles is completed \n",
      "Processing files in : ./data/comp.graphics\n",
      "Processing of 1100 files in ./data/comp.graphics is completed \n",
      "Processing of 1200 files in ./data/comp.graphics is completed \n",
      "Processing of 1300 files in ./data/comp.graphics is completed \n",
      "Processing of 1400 files in ./data/comp.graphics is completed \n",
      "Processing of 1500 files in ./data/comp.graphics is completed \n",
      "Processing of 1600 files in ./data/comp.graphics is completed \n",
      "Processing of 1700 files in ./data/comp.graphics is completed \n",
      "Processing of 1800 files in ./data/comp.graphics is completed \n",
      "Processing of 1900 files in ./data/comp.graphics is completed \n",
      "Processing of 2000 files in ./data/comp.graphics is completed \n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_mapper = {}\n",
    "\n",
    "root_dirs = ['./data/rec.motorcycles', './data/comp.graphics']\n",
    "doc_ID = 0\n",
    "positional_dict = {}\n",
    "for fold in root_dirs:\n",
    "    print('Processing files in : {}'.format(fold))\n",
    "    for file in os.listdir(fold):\n",
    "        path = os.path.join(fold,file)\n",
    "        \n",
    "        with open(path , 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            positional_dict = process_text(lines,positional_dict,doc_ID)\n",
    "            file_mapper[doc_ID] = path\n",
    "            doc_ID += 1\n",
    "            if doc_ID % 100 == 0:\n",
    "                print('Processing of {} files in {} is completed '.format(doc_ID,fold))\n",
    "\n",
    "# pickling the dictionary\n",
    "pickle_data('positional_dict.pkl',positional_dict)\n",
    "pickle_data('pos_file_mapper.pkl',file_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26205"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_dict = load_pickle_data('positional_dict.pkl')\n",
    "file_mapper = load_pickle_data('pos_file_mapper.pkl')\n",
    "len(positional_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_list(word):\n",
    "    '''\n",
    "    This will retrieve postings list of given token if exists\n",
    "    '''\n",
    "    ans = []\n",
    "    if word in positional_dict.keys():\n",
    "        #print('Term {} is present in the dictionary'.format(word))\n",
    "        ans =  positional_dict[word]\n",
    "    else:\n",
    "        print('Term : {} not present in dictionary'.format(word))\n",
    "    return ans\n",
    "\n",
    "\n",
    "def positional_intersect(pos_list_1,pos_list_2,k):\n",
    "    ans = []\n",
    "    for file_id in pos_list_1.keys():\n",
    "        if file_id in pos_list_2.keys():\n",
    "            list_1 = pos_list_1[file_id]\n",
    "            list_2 = pos_list_2[file_id]\n",
    "            \n",
    "            \n",
    "            for pos1 in list_1:\n",
    "                for pos2 in list_2 :\n",
    "                    if pos2 - pos1 == k : #or pos1 - pos2 == k :\n",
    "                        if file_id not in ans:\n",
    "                            ans.append(file_id)\n",
    "                            #print('file found.')\n",
    "                        break\n",
    "    return ans  \n",
    "    \n",
    "\n",
    "def process_query(query):\n",
    "    results = []\n",
    "    query = process_words(query)\n",
    "    print('final query after preprocessing :')\n",
    "    print(query)\n",
    "    for i in range(len(query)):\n",
    "        j = i + 1\n",
    "        pos_list_1 = retrieve_list(query[i])\n",
    "        while j < len(query):\n",
    "            pos_list_2 = retrieve_list(query[j])\n",
    "            \n",
    "            swap = False\n",
    "            if pos_list_1[0] > pos_list_2[0]:\n",
    "                #print('Swapping')\n",
    "                pos_list_1, pos_list_2 = pos_list_2, pos_list_1\n",
    "                swap = True\n",
    "            if swap :\n",
    "                k = i - j\n",
    "            else :\n",
    "                k = j - i\n",
    "            # Small postings list is always first one for optimisation\n",
    "            ans = positional_intersect(pos_list_1[1], pos_list_2[1], k)\n",
    "            results.append(ans)\n",
    "            j += 1\n",
    "    return results\n",
    "    \n",
    "def construct_dict(lists):\n",
    "    results = {}\n",
    "    for lis in lists:\n",
    "        for ele in lis:\n",
    "            if ele in results.keys():\n",
    "                results[ele] += 1\n",
    "            else:\n",
    "                results[ele] = 1\n",
    "    return results\n",
    "\n",
    "def print_output(results):\n",
    "    \n",
    "    print('-----------------------')\n",
    "    print('The most probable files for given phrasal query in descending order is : ')\n",
    "    i  = 1\n",
    "    total = sum(results.values())\n",
    "    while len(results) > 0:\n",
    "        count = max(results.values())\n",
    "        prob = (count / total)\n",
    "        for key in results.keys():\n",
    "            if results[key] == count:\n",
    "                print('file {} is {} with probability : {} '.format(i,file_mapper[key], prob))\n",
    "                i += 1\n",
    "                break\n",
    "        del results[key]\n",
    "        if i == 5 :\n",
    "            break\n",
    "        \n",
    "def read_query():\n",
    "    query = input().split()\n",
    "    results = process_query(query)\n",
    "    results = construct_dict(results)\n",
    "    print_output(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no larger tank available for the Hawk\n",
      "final query after preprocessing :\n",
      "['there', 'is', 'no', 'larger', 'tank', 'available', 'for', 'the', 'hawk']\n",
      "-----------------------\n",
      "The most probable files for given phrasal query in descending order is : \n",
      "file 1 is ./data/rec.motorcycles\\103128 with probability : 0.02447980416156671 \n",
      "file 2 is ./data/comp.graphics\\38403 with probability : 0.009791921664626682 \n",
      "file 3 is ./data/comp.graphics\\39078 with probability : 0.009791921664626682 \n",
      "file 4 is ./data/comp.graphics\\39638 with probability : 0.009791921664626682 \n",
      "file 5 is ./data/comp.graphics\\38376 with probability : 0.0073439412484700125 \n",
      "file 6 is ./data/comp.graphics\\38377 with probability : 0.0073439412484700125 \n",
      "file 7 is ./data/comp.graphics\\38851 with probability : 0.0073439412484700125 \n",
      "file 8 is ./data/comp.graphics\\38853 with probability : 0.0073439412484700125 \n",
      "file 9 is ./data/rec.motorcycles\\102616 with probability : 0.006119951040391677 \n"
     ]
    }
   ],
   "source": [
    "read_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
